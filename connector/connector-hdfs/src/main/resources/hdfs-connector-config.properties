# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

connector.name = HDFS Connector

# Link Config
linkConfig.label = HDFS cluster
linkConfig.help = Contains configuration required to connect to your HDFS cluster.

linkConfig.uri.label = URI
linkConfig.uri.example = hdfs://nn1.example.com/
linkConfig.uri.help = Namenode URI for your cluster.

linkConfig.confDir.label = Conf directory
linkConfig.confDir.example = /etc/hadoop/conf/
linkConfig.confDir.help = Directory on Sqoop server machine with hdfs configuration files (hdfs-site.xml, \
  ...). This connector will load all files ending with -site.xml.

linkConfig.configOverrides.label = Additional configs:
linkConfig.configOverrides.example = custom.key=custom.value
linkConfig.configOverrides.help = Additional configuration that will be set on HDFS Configuration object, \
  possibly overriding any keys loaded from configuration files.

# To Job Config
toJobConfig.label = Target configuration
toJobConfig.help = Configuration describing where and how the resulting data should be stored.

toJobConfig.outputFormat.label = File format
toJobConfig.outputFormat.example = PARQUET_FILE
toJobConfig.outputFormat.help = File format that should be used for transferred data.

toJobConfig.compression.label = Compression codec
toJobConfig.compression.example = SNAPPY
toJobConfig.compression.help = Compression codec that should be use to compress transferred data.

toJobConfig.customCompression.label = Custom codec
toJobConfig.customCompression.example = org.apache.hadoop.io.compress.SnappyCodec
toJobConfig.customCompression.help = Fully qualified class name with Hadoop codec implementation that should be \
  used if none of the build-in options are suitable.

toJobConfig.outputDirectory.label = Output directory
toJobConfig.outputDirectory.example = /user/jarcec/output-dir
toJobConfig.outputDirectory.help = HDFS directory where transferred data will be written to.

toJobConfig.appendMode.label = Append mode
toJobConfig.appendMode.example = true
toJobConfig.appendMode.help = If set to false, job will fail if output directory already exists. If set to true \
  then imported data will be stored to already existing and possibly non empty directory.

toJobConfig.deleteOutputDirectory.label = Delete output directory
toJobConfig.deleteOutputDirectory.example = true
toJobConfig.deleteOutputDirectory.help = If set to false, job will fail if output directory already exists. If set to true \
  then existing output directory will be deleted before job execution.

toJobConfig.overrideNullValue.label = Override null value
toJobConfig.overrideNullValue.example = true
toJobConfig.overrideNullValue.help = If set to true, then the null value will be overridden with the value set in \
                                     Null value.

toJobConfig.nullValue.label = Null value
toJobConfig.nullValue.example = \N
toJobConfig.nullValue.help = For file formats that doesn't have native representation of NULL (as for example text file) \
   use this particular string to encode NULL value.


incremental.label = Incremental import
incremental.help = Information relevant for incremental reading from HDFS.

incremental.incrementalType.label = Incremental type
incremental.incrementalType.example = NEW_FILES
incremental.incrementalType.help = Type of incremental import.

incremental.lastImportedDate.label = Last imported date
incremental.lastImportedDate.example = 1987-02-02 13:15:27
incremental.lastImportedDate.help = Datetime stamp of last read file. Next job execution will read only files that have been \
  created after this point in time.

# From Job Config
fromJobConfig.label = Input configuration
fromJobConfig.help = Specifies information required to get data from HDFS.

fromJobConfig.inputDirectory.label = Input directory
fromJobConfig.inputDirectory.example = /user/jarcec/input-dir
fromJobConfig.inputDirectory.help = Input directory containing files that should be transferred.

fromJobConfig.overrideNullValue.label = Override null value
fromJobConfig.overrideNullValue.example = true
fromJobConfig.overrideNullValue.help = If set to true, then the null value will be overridden with the value set in \
                                     Null value.

fromJobConfig.nullValue.label = Null value
fromJobConfig.nullValue.example = \N
fromJobConfig.nullValue.help = For file formats that doesn't have native representation of NULL (as for example text file) \
   use this particular string to decode NULL value.
